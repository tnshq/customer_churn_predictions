{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5db8f6ef",
   "metadata": {},
   "source": [
    "# Churn Prediction Model with SHAP Explainability\n",
    "\n",
    "This notebook builds machine learning models to predict customer churn, evaluates model performance, and uses SHAP (SHapley Additive exPlanations) to explain predictions and understand feature importance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681106e7",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccee973e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import bz2\n",
    "import warnings\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                            f1_score, roc_auc_score, roc_curve, confusion_matrix,\n",
    "                            classification_report)\n",
    "import shap\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4c89dc",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa942e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('../data/WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
    "\n",
    "# Data preparation\n",
    "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
    "df['TotalCharges'].fillna(df['MonthlyCharges'], inplace=True)\n",
    "df['SeniorCitizen'] = df['SeniorCitizen'].map({0: 'No', 1: 'Yes'})\n",
    "\n",
    "print(f\"Dataset Shape: {df.shape}\")\n",
    "print(f\"Churn Distribution:\\n{df['Churn'].value_counts()}\\n\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6357aa14",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering and Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f436e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop customerID\n",
    "df_model = df.drop(['customerID'], axis=1)\n",
    "\n",
    "# Encode target variable\n",
    "df_model['Churn'] = df_model['Churn'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "categorical_cols = df_model.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_cols = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
    "\n",
    "print(f\"Categorical features: {len(categorical_cols)}\")\n",
    "print(f\"Numerical features: {len(numerical_cols)}\")\n",
    "\n",
    "# Label encode categorical features\n",
    "label_encoders = {}\n",
    "df_encoded = df_model.copy()\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df_encoded[col] = le.fit_transform(df_encoded[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "print(f\"\\n✓ Encoded {len(categorical_cols)} categorical features\")\n",
    "print(f\"✓ Final dataset shape: {df_encoded.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82def60",
   "metadata": {},
   "source": [
    "## 4. Train-Test Split and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40564d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split features and target\n",
    "X = df_encoded.drop('Churn', axis=1)\n",
    "y = df_encoded['Churn']\n",
    "\n",
    "# Train-test split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])\n",
    "X_test[numerical_cols] = scaler.transform(X_test[numerical_cols])\n",
    "\n",
    "print(\"Dataset Split:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Training set: {X_train.shape[0]:,} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Testing set:  {X_test.shape[0]:,} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Features: {X_train.shape[1]}\")\n",
    "print(f\"\\nChurn distribution in training set:\")\n",
    "print(y_train.value_counts(normalize=True) * 100)\n",
    "print(f\"\\nChurn distribution in testing set:\")\n",
    "print(y_test.value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc0fb1c",
   "metadata": {},
   "source": [
    "## 5. Build and Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705f68d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred),\n",
    "        'roc_auc': roc_auc_score(y_test, y_pred_proba),\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    print(f\"✓ {name} trained successfully\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "performance_df = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'Accuracy': [results[m]['accuracy'] for m in results],\n",
    "    'Precision': [results[m]['precision'] for m in results],\n",
    "    'Recall': [results[m]['recall'] for m in results],\n",
    "    'F1-Score': [results[m]['f1'] for m in results],\n",
    "    'ROC-AUC': [results[m]['roc_auc'] for m in results]\n",
    "})\n",
    "performance_df = performance_df.round(4)\n",
    "print(performance_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2146c045",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6615ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# ROC Curve comparison\n",
    "for name in results:\n",
    "    fpr, tpr, _ = roc_curve(y_test, results[name]['probabilities'])\n",
    "    auc = results[name]['roc_auc']\n",
    "    axes[0].plot(fpr, tpr, label=f'{name} (AUC = {auc:.3f})', linewidth=2.5)\n",
    "\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', label='Random Classifier', linewidth=1.5)\n",
    "axes[0].set_xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('ROC Curves Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Metrics comparison bar plot\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.25\n",
    "\n",
    "for idx, name in enumerate(results.keys()):\n",
    "    values = [results[name]['accuracy'], results[name]['precision'], \n",
    "             results[name]['recall'], results[name]['f1'], results[name]['roc_auc']]\n",
    "    axes[1].bar(x + idx*width, values, width, label=name, alpha=0.8)\n",
    "\n",
    "axes[1].set_xlabel('Metrics', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Model Performance Metrics', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xticks(x + width)\n",
    "axes[1].set_xticklabels(metrics)\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "axes[1].set_ylim([0, 1.0])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../static/images/model_evaluation.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9e3d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, name in enumerate(results.keys()):\n",
    "    cm = confusion_matrix(y_test, results[name]['predictions'])\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n",
    "               cbar=True, square=True, linewidths=1, linecolor='black')\n",
    "    axes[idx].set_xlabel('Predicted', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Actual', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_title(f'{name}\\nConfusion Matrix', fontsize=13, fontweight='bold')\n",
    "    axes[idx].set_xticklabels(['No Churn', 'Churn'])\n",
    "    axes[idx].set_yticklabels(['No Churn', 'Churn'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../static/images/confusion_matrices.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3d38fa",
   "metadata": {},
   "source": [
    "## 7. Hyperparameter Tuning - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba0ce1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for Random Forest\n",
    "print(\"Performing Grid Search for Random Forest...\")\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='roc_auc', \n",
    "                          verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"BEST HYPERPARAMETERS\")\n",
    "print(\"=\" * 80)\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Train final model with best parameters\n",
    "best_rf = grid_search.best_estimator_\n",
    "y_pred_best = best_rf.predict(X_test)\n",
    "y_pred_proba_best = best_rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"BEST RANDOM FOREST PERFORMANCE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_pred_best):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_best):.4f}\")\n",
    "print(f\"Recall:    {recall_score(y_test, y_pred_best):.4f}\")\n",
    "print(f\"F1-Score:  {f1_score(y_test, y_pred_best):.4f}\")\n",
    "print(f\"ROC-AUC:   {roc_auc_score(y_test, y_pred_proba_best):.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CLASSIFICATION REPORT\")\n",
    "print(\"=\" * 80)\n",
    "print(classification_report(y_test, y_pred_best, target_names=['No Churn', 'Churn']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5c0c83",
   "metadata": {},
   "source": [
    "## 8. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea76e2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance from Random Forest\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': best_rf.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "# Plot top 15 features\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = feature_importance.head(15)\n",
    "plt.barh(range(len(top_features)), top_features['Importance'], color='steelblue', alpha=0.8)\n",
    "plt.yticks(range(len(top_features)), top_features['Feature'])\n",
    "plt.xlabel('Importance Score', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Features', fontsize=12, fontweight='bold')\n",
    "plt.title('Top 15 Feature Importances (Random Forest)', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../static/images/feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 15 Most Important Features:\")\n",
    "print(\"=\" * 80)\n",
    "for idx, row in feature_importance.head(15).iterrows():\n",
    "    print(f\"{row['Feature']:30s}: {row['Importance']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b5cea1",
   "metadata": {},
   "source": [
    "## 9. SHAP Analysis for Model Explainability\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) values provide a unified measure of feature importance and show how each feature contributes to individual predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a367be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SHAP explainer\n",
    "print(\"Creating SHAP explainer (this may take a few minutes)...\")\n",
    "explainer = shap.TreeExplainer(best_rf)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# For binary classification, we get shap values for both classes\n",
    "# We'll use class 1 (churn) for visualization\n",
    "if isinstance(shap_values, list):\n",
    "    shap_values_churn = shap_values[1]\n",
    "else:\n",
    "    shap_values_churn = shap_values\n",
    "\n",
    "print(\"✓ SHAP explainer created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5ca610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Summary Plot - Feature Importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(shap_values_churn, X_test, plot_type=\"bar\", show=False)\n",
    "plt.title('SHAP Feature Importance (Mean Absolute SHAP Values)', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.xlabel('Mean |SHAP Value|', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../static/images/shap_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# SHAP Summary Plot - Feature Impact\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(shap_values_churn, X_test, show=False)\n",
    "plt.title('SHAP Feature Impact on Churn Prediction', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.xlabel('SHAP Value (Impact on Model Output)', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../static/images/shap_summary.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ SHAP visualizations created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898e6e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Dependence Plots for top features\n",
    "top_4_features = feature_importance.head(4)['Feature'].tolist()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature in enumerate(top_4_features):\n",
    "    shap.dependence_plot(feature, shap_values_churn, X_test, \n",
    "                        ax=axes[idx], show=False)\n",
    "    axes[idx].set_title(f'SHAP Dependence: {feature}', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../static/images/shap_dependence.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ SHAP dependence plots created for top {len(top_4_features)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b380eb80",
   "metadata": {},
   "source": [
    "## 10. Save Models and Artifacts for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd04db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "with open('../model.pkl', 'wb') as f:\n",
    "    pickle.dump(best_rf, f)\n",
    "print(\"✓ Best Random Forest model saved to '../model.pkl'\")\n",
    "\n",
    "# Save the SHAP explainer (compressed)\n",
    "with bz2.BZ2File('../explainer.bz2', 'w') as f:\n",
    "    pickle.dump(explainer, f)\n",
    "print(\"✓ SHAP explainer saved to '../explainer.bz2'\")\n",
    "\n",
    "# Save scaler and encoders\n",
    "artifacts = {\n",
    "    'scaler': scaler,\n",
    "    'label_encoders': label_encoders,\n",
    "    'feature_names': X.columns.tolist(),\n",
    "    'numerical_cols': numerical_cols,\n",
    "    'categorical_cols': categorical_cols\n",
    "}\n",
    "\n",
    "with open('../preprocessing_artifacts.pkl', 'wb') as f:\n",
    "    pickle.dump(artifacts, f)\n",
    "print(\"✓ Preprocessing artifacts saved to '../preprocessing_artifacts.pkl'\")\n",
    "\n",
    "# Save model performance metrics\n",
    "performance_summary = {\n",
    "    'test_accuracy': accuracy_score(y_test, y_pred_best),\n",
    "    'test_precision': precision_score(y_test, y_pred_best),\n",
    "    'test_recall': recall_score(y_test, y_pred_best),\n",
    "    'test_f1': f1_score(y_test, y_pred_best),\n",
    "    'test_roc_auc': roc_auc_score(y_test, y_pred_proba_best),\n",
    "    'best_params': grid_search.best_params_,\n",
    "    'feature_importance': feature_importance.to_dict('records')\n",
    "}\n",
    "\n",
    "with open('../model_performance.pkl', 'wb') as f:\n",
    "    pickle.dump(performance_summary, f)\n",
    "print(\"✓ Model performance summary saved to '../model_performance.pkl'\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ALL MODELS AND ARTIFACTS SAVED SUCCESSFULLY\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nFiles created:\")\n",
    "print(\"  - model.pkl (Random Forest classifier)\")\n",
    "print(\"  - explainer.bz2 (SHAP explainer)\")\n",
    "print(\"  - preprocessing_artifacts.pkl (scalers and encoders)\")\n",
    "print(\"  - model_performance.pkl (metrics and parameters)\")\n",
    "print(\"\\nThe Flask app can now use these trained models for predictions!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
